{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TT_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO6/Vqt7phi75+c8pKP3gNz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itzpankajpanwar/Tree-transform/blob/main/TT_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAuShGrgtrxi",
        "outputId": "b865403b-3418-406f-a192-027a6ed211ba"
      },
      "source": [
        "!pip install transformers\n",
        "!pip install sentencepiece"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 7.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 55.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 58.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting sentencepiece\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.6MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.95\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EnAyMw2m0HA",
        "outputId": "09cc7bd7-4b8b-4293-80aa-ec1484899fea"
      },
      "source": [
        "!git clone https://github.com/itzpankajpanwar/Tree-transform.git"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Tree-transform'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 21 (delta 2), reused 13 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (21/21), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnnBZVzPmz1O"
      },
      "source": [
        "Main Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S3NPX6Wb6feK"
      },
      "source": [
        "import numpy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/Tree-transform\")\n",
        "from attention import *\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "    \n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, layer, N, d_model, vocab_size, word_embed):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.word_embed = word_embed\n",
        "        self.layers = clones(layer, N)\n",
        "        self.norm = LayerNorm(layer.size)\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, inputs, mask):\n",
        "        break_probs = []\n",
        "        x = self.word_embed(inputs)\n",
        "        group_prob = 0.\n",
        "        for layer in self.layers:\n",
        "            x,group_prob,break_prob = layer(x, mask,group_prob)\n",
        "            break_probs.append(break_prob)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        break_probs = torch.stack(break_probs, dim=1)\n",
        "        return self.proj(x),break_probs\n",
        "\n",
        "\n",
        "    def masked_lm_loss(self, out, y):\n",
        "        fn = CrossEntropyLoss(ignore_index=-1)\n",
        "        return fn(out.view(-1, out.size()[-1]), y.view(-1))\n",
        "\n",
        "\n",
        "    def next_sentence_loss(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    \"Encoder is made up of self-attn and feed forward (defined below)\"\n",
        "    def __init__(self, size, self_attn, feed_forward, group_attn, dropout):\n",
        "        super(EncoderLayer, self).__init__()\n",
        "        self.self_attn = self_attn\n",
        "        self.feed_forward = feed_forward\n",
        "        self.group_attn = group_attn\n",
        "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
        "        self.size = size\n",
        "\n",
        "    def forward(self, x, mask, group_prob):\n",
        "        group_prob,break_prob = self.group_attn(x, mask, group_prob)\n",
        "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, group_prob, mask))\n",
        "        return self.sublayer[1](x, self.feed_forward), group_prob, break_prob"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oin3-UaymCg",
        "outputId": "0bd19fd7-bce8-49c1-c756-8c37de9f8a40"
      },
      "source": [
        "!python3 \"/content/Tree-transform/main.py\" -train -model_dir model_file -num_step 60000"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "vocab_size: 13860\n",
            "/content/Tree-transform/solver.py:44: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "  nn.init.xavier_uniform(p)\n",
            "total_param_num: 50994724\n",
            "\n",
            "start epo 0!!!!!!!!!!!!!!!!\n",
            "\n",
            "saving!!!!\n",
            "Epoch Step: 1 Loss: 9.452121 Total Time: 1.335392 Step Time: 0.922801\n",
            "\n",
            "Epoch Step: 201 Loss: 7.142012 Total Time: 88.486306 Step Time: 87.488985\n",
            "\n",
            "Epoch Step: 401 Loss: 6.568196 Total Time: 88.928025 Step Time: 87.925942\n",
            "\n",
            "Epoch Step: 601 Loss: 6.376680 Total Time: 89.119741 Step Time: 88.117886\n",
            "\n",
            "\n",
            "finish epo 0, time 286.171292!!!!!!!!!!!!!!!\n",
            "\n",
            "\n",
            "start epo 1!!!!!!!!!!!!!!!!\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}